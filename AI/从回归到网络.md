# 从回归到网络

神经网络是一群回归单元的组合。因此，整体流程是从回归到网络的学习。

首先建模，然后是更新模型的参数（梯度下降法），找到最优解。

## 线性归回：

- 建模： $h_\theta(x) = \theta_0+\theta_1*x + ...$
- 损失函数：$J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^i)-y^i)^2$

- 梯度更新：$\theta_j := \theta_j-\alpha\frac1m\sum_{i-1}^m((h_\theta(x^i)-y^i)*x_j^i)$

## 逻辑回归：

- 建模（sigmoid）：$h_\theta(x)=\frac1{1+e^{-(\theta^Tx)}}$
- 损失函数：$J(\theta)=-\frac1m\sum_{i=1}^my^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))$
- 梯度更新：$\theta_j := \theta_j - \alpha\frac1m\sum_{i=1}^m(h_\theta(x^i)-y^i)x_j^i$

## 反向传播：

- 建模：神经网络的构建
- 损失函数：一层层向后传播

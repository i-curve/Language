# python 爬虫

## 一、网络请求

1.  urllib

python 自带的网络库, 可以直接使用,但是不是很方便

- request 模块: 网络请求模块, 包含 urlopen(发送请求), Request(构造请求头) 两个类
- error 模块: 处理 request 中出现的异常
- parse 模块: 解析 url

```py
from urllib import request
# 1. request 模块
resp = request.urlopen("http://www.baidu.com")
print(resp.status) # 状态码
print(resp.read().decode('utf-8')) # 读取网页内容

req = request.Request("http://www.baidu.com",method = 'GET')
resp = request.urlopen(req)

# error 模块: 略

# parse 模块:
parse.urlparse("https://baidu.com")
a = {'a':1,'b':2}
parse.urlencode(a)
#==> a=1&b=2
```

2. requests

python 基于 urllib 的第三方网络库

```py
requests.get("http://www.baidu.com")
requests.post()
request.put()
request.patch()
request.delete()
# 参数
# params
# data
# headers
# files: 上传文件
# varify: 是否验证ssl证书
# auth: 身份验证
# proxies: 代理

# 文件上传
file= {'file': open('a.txt','rb')}
requests.post('https://baidu.com',files=file)
```

## 二、页面处理

1. 正则

2. parsel
